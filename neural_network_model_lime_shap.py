# -*- coding: utf-8 -*-
"""Neural_Network_Model_LIME_SHAP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gYpnZXy18kSQdDy4D-jRXLe_ipZ_jIW0

The Iris dataset is popular, and has been used in many python and machine learning projects. It is in the scikit-learn library and uses 150 samples of Iris flowers, 50 samples from different species like setosa,veriscolor, and virginica. The samples include Sepal Length, Sepal width. petal length and petal length. It is a good dataset to use for performance of Machine Learning Models.
"""

#importing Packages
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris

#Downloading Iris Dataset as dataframe
raw_data = load_iris(as_frame=True)
iris_data = raw_data.frame

iris_data.head()

#Viewing the tail end of the data

iris_data.tail()

#Exploring the data more
iris_data.info()

#Remove target variable

#The below code is going to create a new column called class and based on the integer values found in target column
#It is going to map it the target_names which ismore human=-readable and interpretable
iris_data['class'] = iris_data['target'].map(lambda ind: raw_data['target_names'][ind])

#Drop the target column since we created class column
iris_data = iris_data.drop('target',axis=1)

#View the new dataset with class column added and make sure column is dropped

type(iris_data)
print(iris_data.info())
print(iris_data.describe())

iris_data.head()

#Split up X and Y values in Iris Dataset
from sklearn.model_selection import train_test_split
X = iris_data.drop('class',axis=1)
y = iris_data['class']

#Now we need to look at variables and see which ones are highly correlated

#import seaborn package
import seaborn as sns

cormat = X.corr()
round(cormat,2)
sns.heatmap(cormat,annot=True, cmap='RdYlGn')

"""Question 4: For the Iris dataset I did a correlation map to explore the data further and accurately gain insights about the feature variables we used. The columns that we have in our feature list are Petal Width, Petal Length, Sepal Length, and Sepal Width, for a classification model these features would be a great fit to help us train the neural networks model. Moving forward, there needed to be an observation on the correlation between features.

By creating a heatmap on the correlation between features there were some interesting observations. In the analysis it was noticed that there was a high positive correlation between Petal Width and Petal Length (0.96), a high positive correlation between Petal Length and Sepal Length (0.87), and a high positive correlation between Petal Width and Sepal Length (0.81). Usually if independent variables are highly correlated to each other in a model it is called multicollinearity and can reduce interpretability, model overfitting, and under or overestimating parameters. In this case we only have 4 features to use for our model, for now I won’t drop them to see how the model reacts, and if it does interfere with interpretability I will re-do feature selection and drop Petal Length if necessary.

"""

#Split dataset into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

#Set the random number generation seed so that results can be duplicated
np.random.seed(12345)

#fit a neural network model witht the training data
from sklearn.neural_network import MLPClassifier
mlp = MLPClassifier(hidden_layer_sizes=(30,30,30), activation='relu',solver='adam',max_iter=20000)
mlp.fit(X_train.values,y_train)

from sklearn.metrics import classification_report
#Provide quality of fit metrics with the test dataset
y_pred = mlp.predict(X_test)
target_names = set(raw_data['target_names'])
print(X_test.columns)
print(classification_report(y_test, y_pred, target_names=target_names))

#Lime Install
!pip install lime
import lime
from lime import lime_tabular

test_1 = X_test.iloc[14]
print(test_1, y_pred[14])

lime_explainer = lime_tabular.LimeTabularExplainer(
    training_data=np.array(X_train),
    feature_names=X_train.columns,
    class_names=['class_0', 'class_1','class_2'],
    mode='classification'
)
#Explain one instance of the data with lime
lime_exp = lime_explainer.explain_instance(
    data_row=test_1,
    predict_fn=mlp.predict_proba
)
lime_exp.show_in_notebook(show_table=True)

import matplotlib.pyplot as plt

weights = []

#Function to get weights from LIME explanation object
def return_weights(ep):
    exp_list = exp.as_map()[1]
    exp_list = sorted(exp_list, key=lambda x: x[0])
    exp_weight = [x[1] for x in exp_list]
    return exp_weight

print(X_test.columns)
print(len(X_test))

#Iterate over the rows in feature matrix
#and collect the LIME weights
#You can run for more elements----but we pick a max of 25
for x in range(min(len(X_test),25)):
    #Get explanation
    exp = lime_explainer.explain_instance(X_test.iloc[x],
                                 mlp.predict_proba, num_features = len(X_test.columns))
    #Get weights
    exp_weight = return_weights(exp)
    weights.append(exp_weight)

#Create DataFrame of the LIME weights
lime_weights = pd.DataFrame(data=weights,columns=X_test.columns)

#Get absolute value of the mean of LIME weights
abs_mean = lime_weights.abs().mean(axis=0)
abs_mean = pd.DataFrame(data={'feature':abs_mean.index, 'abs_mean':abs_mean})
abs_mean = abs_mean.sort_values('abs_mean')

#Plot abs mean LIME weights
fig, ax = plt.subplots(nrows=1, ncols=1,figsize=(8,8))

y_ticks = range(len(abs_mean))
y_labels = abs_mean.feature
plt.barh(y=y_ticks,width=abs_mean.abs_mean)

plt.yticks(ticks=y_ticks,labels=y_labels,size= 15)
plt.title('')
plt.ylabel('')
plt.xlabel('Mean |Weight|',size=20)

#Call pip install to get shap
!pip install shap
import shap
shap.initjs()

#Create summary plot of the influence of all variables on row 15 of the test dataset

shap_explainer = shap.KernelExplainer(mlp.predict_log_proba, X_train)

shap_values = shap_explainer.shap_values(X_test.iloc[14,:])

shap.force_plot(shap_explainer.expected_value[0], shap_values[0], X_test.iloc[14,:])

#Now we show an interactive plot for all values/samples in the dataset
#to tie out before you need original sample ordering on the x axis and
shap.initjs()
shap_values = shap_explainer.shap_values(X_test)
shap.force_plot(shap_explainer.expected_value[0],shap_values[0], X_test)

"""Question 7: The row I picked was iloc [14] for the row that I wanted to predict on in the Iris dataset. First, we will talk about lime and its output. LIME is a model where it interprets the input data for humans to understand. Even though we used a Blackbox type of model for our Iris dataset, the LIME model will still help breakdown important features to help interpret the model’s predictions.
The Neural Network we used is doing classification on the dataset, so it is important to understand why it made certain predictions. This can help with determining if there is a need to go back and do feature engineering, if the data is too redundant, or if you should adjust hyper parameters and change approach. It is good to understand that LIME provides a local model interpretability approach. LIME modifies a single data sample by tweaking the feature values and observes the resulting impact on the output. This helps with understanding why a certain prediction was made and what caused that said prediction.
LIME explains the output of the feature list and how it had an impact on the prediction of the sample that was given. For instance, iloc [14] the data values were listed as follows sepal length (cm) 5.1, sepal width (cm) 3.8, petal length (cm) 1.5, and petal width (cm) 0.3. LIME model had 3 classes which were Class 0, Class 1, and Class 2. Which would follow the proceeding of Sentosa would be in class 0, versicolor would be in class 1 and virginica would be in class 2. In this case it predicted that class 0 would be the right choice , which was correct because the target in the corresponding row was Setosa. It also noticed that the value of petal width was off and would classify for a different class which it was correct. The petal width was 0.3, but if you look at a setosa petal width in the dataset it is usually around 0.2, It did do a good job classifying features and given the user a better understanding on how to interpret it in this case.
Example:


Now we will talk about SHAP. SHAP values provide a method of fairness and a method of specific allocation properties that are desirable. We use shapely values to apply credit to each feature. That is very important because no other model allocates that type of credit appropriately. It is important to use the SHAP model because since it is derived off of a more accurate mathematical background, it gives us a better depiction of the feature importance in the dataset.
For the Iris dataset I used the shap_explainer on the train data and had it parse through the values for the individual prediction to help understand the model’s decision-making process. It also provides an understanding of the importance of the features for prediction. Once the Shap values and Shap explainer were added, we made a summary plot to see the influence of all the variables on iloc [14] of the test dataset. According to this plot the Petal Length had the most contribution to the prediction, this was also the case with the LIME model analysis.






"""

#Create a summary plot of the influence of all input variables on the test dataset
shap.initjs()

shap_explainer = shap.KernelExplainer(mlp.predict_proba, X_train)

shap_vals = shap_explainer.shap_values(X_test)

shap.summary_plot(shap_vals, X_test, max_display=X_test.shape[1])

"""Question 8: Using LIME weights was good, it helped me work through the contribution of the features in the neural networks model. It also provided me with an understanding of how the model did its decision-making process. LIME weights provide several benefits, it helps identify the most significant features for a prediction. If the model is not predicting the output expected, then LIME weights can help with debugging the model for the most influential feature in each prediction.
The weights help with a granular understanding of the features in a prediction, for the Iris dataset it helped me pinpoint where the most influential features were that was driving the neural networks decisions. In this case the weights gave me a better picture of the features and their impact than the explainer did on its own. We collected the mean weight of each feature and then ranked them on a horizontal bar plot in matplotlib.
From the analysis of the visual, it displayed petal length as its most influential value with a mean weight around 0.30. Then it had petal width as the second most influential feature with a mean weight around 0.10. Next it was sepal length with a mean weight around 0.07, and lastly it was sepal width which had a mean weight around 0.01. This was a good representation of most influential features and gave me a better understanding of the prediction and the decision-making process of the model. It also let me know that the petal length has a huge impact on the dataset and was a feature I thought about dropping in the dataset.




Using the SHAP summary plot helped me gain insights to the model’s input features as well. The summary plots. Which offers a comprehensive approach to the model in detailing and ranking the dataset by most influential features. By aggregating the values across the dataset, it visualizes their distribution horizontally, just like we did with the LIME visualization but instead of using matplotlib it is in one line of code in the shap package.
This plot helps identify the values in a way where you can interpret them from the ones that have a positive influence on the ones that have a negative influence or less of an impact on the dataset. This is also a great way to visualize it because you get to understand this at a global level of the dataset.
When applying this model to the Iris dataset not only did it distribute the most influential values for the dataset globally, but it also displayed the impact it had on each class in a stacked bar chart representation. The Petal Length had the most impact in each class and had the most impact in the class 0 or the setosa class. The petal width had the most impact in the setosa class as well but had a lot of impact in other classes but not as much as petal length. The sepal length had the most impact in class 2, the versicolor class, and no impact in class 0, and lastly the sepal width impacted class 0 the most. Between the two visuals I liked and understood the shap summary plot the most. It gave a better detail of the most influential features throughout each class and how they were distributed. Comparatively, they both display the same type of information and are both concurrent in rankings of most influential features. Both SHAP and LIME made it a lot easier to understand predictions at a local and global level.

"""

